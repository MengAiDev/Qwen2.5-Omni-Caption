{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":317951,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":268282,"modelId":289307}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mengaidev/qwen2-5-omni-caption?scriptVersionId=265999934\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Qwen2.5 Omni finetune\n\nGoal: Use clotho dataset to finetune qwen2.5-omni-3b.","metadata":{}},{"cell_type":"markdown","source":"## Data preparation","metadata":{}},{"cell_type":"code","source":"!wget https://zenodo.org/records/4783391/files/clotho_audio_development.7z\n!wget https://zenodo.org/records/4783391/files/clotho_captions_development.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install py7zr\n\nimport py7zr\n\n# Extract entire archive\nwith py7zr.SevenZipFile('clotho_audio_development.7z', mode='r') as z:\n    z.extractall('../temp')\n\n!rm clotho_audio_development.7z","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, let's generate the `train.jsonl`","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport json\n\ndef csv_to_jsonl_conversation(csv_file_path, jsonl_file_path, base_path=\"/kaggle/temp/development\"):\n    \"\"\"\n    将CSV文件转换为对话格式的JSONL文件\n    \n    参数:\n    csv_file_path: 输入的CSV文件路径\n    jsonl_file_path: 输出的JSONL文件路径\n    base_path: 要添加到音频文件前的基路径\n    \"\"\"\n    \n    df = pd.read_csv(csv_file_path)\n    \n    with open(jsonl_file_path, 'w', encoding='utf-8') as f:\n        # 遍历每一行（跳过表头）\n        for index, row in df.iterrows():\n            # 构建完整的音频文件路径\n            audio_filename = f\"{base_path}/{row.iloc[0]}\"\n            \n            # 创建对话格式的JSON对象\n            json_obj = {\n                \"messages\": [\n                    {\n                        \"role\": \"user\",\n                        \"content\": \"<audio>What did the audio say?\"\n                    },\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": row.iloc[1]  # 使用CSV第二列的caption\n                    }\n                ],\n                \"audios\": [audio_filename]\n            }\n            \n            # 写入JSONL文件\n            f.write(json.dumps(json_obj, ensure_ascii=False) + '\\n')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv_file = \"clotho_captions_development.csv\"\njsonl_file = \"train.jsonl\"\n    \ncsv_to_jsonl_conversation(csv_file, jsonl_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Finetune","metadata":{}},{"cell_type":"markdown","source":"### Installation","metadata":{}},{"cell_type":"code","source":"%%capture\n\n!pip install --upgrade uv\n!uv pip install ms-swift -U\n!uv pip install --upgrade numpy scikit-learn --force-reinstall\n!uv pip install --upgrade torch torchvision --force-reinstall\n!uv pip install qwen-omni-utils","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Let's start training!","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\n\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.reset_peak_memory_stats()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\nimport os\nos.environ[\"MAX_PIXELS\"]=\"1003520\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!swift sft --model Qwen/Qwen2.5-Omni-3B --dataset train.jsonl --train_type lora --torch_dtype bfloat16 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --learning_rate 1e-4 --lora_rank 8 --lora_alpha 32 --target_modules all-linear --freeze_vit true --gradient_accumulation_steps 4 --eval_steps 100 --save_steps 100 --save_total_limit 5 --logging_steps 10 --max_length 1024 --output_dir qwen_caption_output --warmup_ratio 0.05 --dataloader_num_workers 4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Unfortunately\n\nWe cannot finish the training, it's end with 1100+ steps.","metadata":{}}]}